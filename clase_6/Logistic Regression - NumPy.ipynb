{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de éste ejercicio es que implementen paso a paso los building blocks del modelo de regresión logística, para finalmente crear una clase del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos las Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de Building Blocks del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se deberán implementar paso a paso los distintos bloques de código que conforman el modelo, junto con algunas funciones auxiliares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar la función: $g(z) = \\frac{1}{1 + e^{-z}}$ en NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "     return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar la función de costo: $J(w) = \\frac{1}{n}\\sum_{i=1}^{n}L\\left ( \\hat{y},y \\right )= \\frac{1}{n}\\sum_{i=1}^{n}\\left [y^{(i)}log(\\hat{y}^{(i)})+ (1-y^{(i)})log(1-\\hat{y}^{(i)}) \\right ]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_entropy(Y, A, n):\n",
    "    return np.sum(np.multiply(Y,np.log(A)) + np.multiply((1 - A), (1 - Y)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar el gradiente de la función costo respecto de los parámetros: $\\frac{\\partial J(w)}{\\partial w} = \\frac{1}{n}\\sum_{i=1}^{n}\\left ( \\hat{y}^{i}-y^{i}\\right )\\bar{x}^i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz = A-Y\n",
    "dw = (1/n)*np.dot(X,dz.T)\n",
    "db = (1/m)*np.sum(dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar normalización Z-score de las features de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train_nonull.npy', 'rb') as f:\n",
    "    X_train_nonull = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y_train_nonull.npy', 'rb') as f:\n",
    "    y_train_nonull = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stand = (X_train_nonull - np.mean(X_train_nonull))/np.std(X_train_nonull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_stand.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas (Precision, Recall y Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar las métricas en NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementar función fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizas los bloques anteriores, junto con la implementación en NumPy del algoritmo Mini-Batch gradient descent, para crear la función fit de nuestro modelo de regresión logística. Cada un determinado número de epochs calculen el loss, almacénenlo en una lista y hagan un log de los valores. La función debe devolver los parámetros ajustados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(dim):\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0  \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_update(W,b, dw, db, states, lr):\n",
    "  # hyper-param typical values: learning_rate=0.01, momentum=0.9\n",
    "    W_ant = W\n",
    "    b_ant = b\n",
    "    W = W + hyper_params['momentum'] * states[0] - lr * dw\n",
    "    b = b + hyper_params['momentum'] * states[1] - lr * db\n",
    "    states = [W-W_ant, b-b_ant]\n",
    "    return W,b, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def fit(X_train, y_train, lr, batch, epochs, momentum_rate):\n",
    "\n",
    "    w, b = initialize(X_train.shape[1])\n",
    "    \n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "    states = np.zeros((m,1))\n",
    "    \n",
    "    #Estandarizo los datos\n",
    "    X_train_stand = (X_train - np.mean(X_train))/np.std(X_train)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        idx = np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "\n",
    "        batch_size = int(len(X_train) / batch)\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            end = i + batch_size if i + batch_size <= len(X_train) else len(X_train)\n",
    "            batch_X = X_train[i: end]\n",
    "            batch_y = y_train[i: end]\n",
    "            \n",
    "            z = np.dot(X_train,w) + b\n",
    "            A = sigmoid(z)\n",
    "            print(w.shape)\n",
    "            cost = cost_entropy(y_train, A, n)\n",
    "\n",
    "            dz = A-y_train\n",
    "            dw = (1/n)*np.dot(X_train,dz.T)\n",
    "            db = (1/m)*np.sum(dz)\n",
    "\n",
    "            if momentum_rate > 0:\n",
    "                w, b, states = momentum_update(W,b, dw, db, states, 0.9)\n",
    "            else:\n",
    "                w = w-lr*dw\n",
    "                b = b-lr*db\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 1)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.08 GiB for an array with shape (30837, 30837) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-7d227c01fcaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_nonull\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_nonull\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-799bec255907>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(X_train, y_train, lr, batch, epochs, momentum_rate)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcost_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mdz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-6502aed5a067>\u001b[0m in \u001b[0;36mcost_entropy\u001b[1;34m(Y, A, n)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcost_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 7.08 GiB for an array with shape (30837, 30837) and data type float64"
     ]
    }
   ],
   "source": [
    "fit(X_train_nonull, y_train_nonull, 0.01, 16, 100, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementar función predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar la función predict usando los parámetros calculados y la función sigmoid. Prestar atención a las transformaciones de los datos de entrada. Asimismo, se debe tomar una decisión respecto de los valores de salida como: $p\\geq 0.5 \\to 1, p<0.5 \\to 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def predict(self, X):\n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armar una clase LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armar una clase LogisticRegression que herede de BaseModel y tenga la siguiente estructura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(BaseModel):\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return NotImplemented\n",
    "\n",
    "    def fit(self, X, y, lr, b, epochs, bias=True):\n",
    "        #self.model = W\n",
    "        return NotImplemented\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testear con Datasets sintéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería Scikit-Learn tiene una función make_classification que nos permite armar datasets de prueba para problemas de clasificación. Prueben con datasets que tengan varios clusters por clase, que tengan menor o mayor separación y calculen las métricas en cada caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "# X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
